---
title: "Estimación del IMAE"
author: "Daniel Vásquez"
#date: "2024-02-16"
output: html_document
lang: es-ES
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Modelos ARIMA(p,d,q)

```{r, message=FALSE, warning=FALSE, echo=FALSE}
library(forecast)
library(ggplot2)
library(TSA)
library(tseries)  
library(urca)
library(lmtest)
library(readxl)
library(tsoutliers) #para tratar los datos atípicos

data_excel <- read_xlsx("imae_variacion_acumulada.xlsx")
#datos completos
data_imae <- ts(data_excel$IMAE, start = c(2001,01), frequency = 12)

#tomando la serie desde enero 2000 hasta diciembre 2022
imae <- ts(data_excel$IMAE, start = c(2001, 01), end = c(2022,12), frequency = 12)
```

```{r, echo=FALSE}
#Funcion solo para ver la descripcion del modelo y test de coeficientes
coef_arima <- function(data, p,d,q){
  modelo <- Arima(data, order = c(p,d,q))
  # Coeficientes: 
  print(modelo)
  # Test de Coeficientes:
  print(coeftest(modelo))

}

#Función para graficar los pronósticos
graficarPred <- function(data_original, modelo, prediccion){
  plot(prediccion, main = "Serie Original vs Ajuste ARIMA", 
     xlab = "Años", ylab = "IMAE" )
  grid(nx = NULL, ny = NULL,
     lty = 2, col = "gray", lwd = 1)
  lines(data_original)
  lines(modelo$fitted, col = "red")
  legend("bottomleft", 
       legend = c("Serie original", "Ajuste ARIMA", "Predicciones"),
         col = c("black", "red", "cyan3"), 
       lty = c(1,1,1), lwd = 2)
  
}

```

Grafico de la serie temporal IMAE - variación acumulada:
```{r}
ggplot(data_imae, aes(x = data_excel$Periodo, y = data_excel$IMAE)) +
  geom_line(color = "blue4") +
  labs(title = "Serie Variación Acumulada - IMAE",
       x = "Años",
       y = "IMAE")
```

Necesitamos verificar si la serie es estacionaria o no, para esto, aplicamos la prueba de Dickey-Fuller.
```{r, echo=FALSE}
# H_0:Hay raíz unitaria     ||  No hay estacionariedad
#H_a: No hay raiz unitaria  ||  Hay estacionariedad     
# Rechazamos H_0 si p-value < alpha ó t < v. critic

### Hay tres tipos de prueba Dickey-FUller:
#Si la serie presenta tendencia, incluir tendencia e intercepto
#si no tiene tendencia y su media no es cero, incluir solo intercepto
#si fluctua en torno a su media (media cero), no incluir nada. 

```


Prueba Dickey-Fuller con intercepto:
```{r}
df1 <- ur.df(imae, type = "drift", lags = 0)
summary(df1) 
```

Prueba Dickey-Fuller sin intercepto y sin tendencia:
```{r}
df2 <- ur.df(imae, type = "none", lags = 0)
summary(df2) 

```

Prueba Dickey-Fuller con tendencia e intercepto:
```{r}
df3 <- ur.df(imae, type = "trend", lags = 0)
summary(df3)
```
Los resultados de estas pruebas nos indican que la serie es no estacionaria.

Ya que los datos son no estacionarios, debemos diferenciar para encontrar el parámetro d.
```{r}
imae_diff <- diff(imae)

```

Una vez diferenciada, necesitamos comprobar si ya es estacionaria con el test Dickey-Fuller nuevamente.
```{r}
adf.test(imae_diff) # Dickey-Fuller por defecto

```

Prueba Dickey-Fuller sin intercepto y sin tendencia:
```{r}
df <- ur.df(imae_diff, type = "none", lags = 0)
summary(df) 
```
El resultado de la prueba indica estacionariedad. Por ende, bajo este análisis, la serie es integrada de orden 1 (d=1).


Procedemos a examinar el ACF y PACF de la serie diferenciada:
```{r}
par(mfrow = c(1, 2))
acf(imae_diff, main = "ACF de las diferencias", lag.max = 100)
pacf(imae_diff, main = "PACF de las diferencias", lag.max = 100)
```

La PACF muestra significancia en los rezagos, 1, 2, 3 y 4, en el caso de la ACF, los rezagos 1 y 2 son significantes . Estos valores son los candidatos para los parámetros p y q. 


### Encontrando el mejor modelo ARIMA:

```{r, echo=FALSE, message=FALSE, warning=FALSE}
# for (i in 0:4) {
#   for (j in 0:4) {
#     coef_arima(imae, i,1,j)
#   }
# }

# Modelos que muestran mejor AIC y la estimación de los coeficientes es buena.
arima011 <- Arima(imae, order = c(0,1,1))
arima110 <- Arima(imae, order = c(1,1,0))
arima111 <- Arima(imae, order = c(1,1,1))
arima212 <- Arima(imae, order = c(2,1,2))
arima313 <- Arima(imae, order = c(3,1,3))
arima414 <- Arima(imae, order = c(4,1,4))

resid011 <- resid(arima011)
resid110 <- resid(arima110)
resid111 <- resid(arima111)
resid212 <- resid(arima212)
resid313 <- resid(arima313)
resid414 <- resid(arima414)

#Prueba Box-Ljung para descartar autocorrelación en los residuos
### H_0: p_0 = p_1 = ... = p_n = 0
#   H_a: p_i != 0 para algún i 
#   rechazamos H_0 si p < 0.05
#    m ~ Ln(264), Q(6) ----> lag = 6
Box.test(resid011, lag = 6, type = "Ljung")
Box.test(resid110, lag = 6, type = "Ljung")
Box.test(resid111, lag = 6, type = "Ljung")
Box.test(resid212, lag = 6, type = "Ljung")
Box.test(resid313, lag = 6, type = "Ljung")
Box.test(resid414, lag = 6, type = "Ljung")

#Las pruebas muestran que no hay correlacion entre los residuos, es decir, son ruido blanco para cada modelo

### Error medio cuadrático y Error medio absoluto
mse <- c(mean(arima011$residuals^2),
          mean(arima110$residuals^2),
          mean(arima111$residuals^2),
          mean(arima212$residuals^2),
          mean(arima313$residuals^2),
          mean(arima414$residuals^2))

mae <- c(mean(abs(resid011)),
         mean(abs(resid110)),
         mean(abs(resid111)),
         mean(abs(resid212)),
         mean(abs(resid313)),
         mean(abs(resid414)))


aic <- c(arima011$aic,
         arima110$aic,
         arima111$aic,
         arima212$aic,
         arima313$aic,
         arima414$aic)

model <- c("ARIMA(0,1,1)",
           "ARIMA(1,1,0)",
           "ARIMA(1,1,1)",
           "ARIMA(2,1,2)",
           "ARIMA(3,1,3)",
           "ARIMA(4,1,4)")

specs_arima <- data.frame(model,aic, mse,mae)

```


Luego de experimentar con los distintos parámetros, encontramos los siguientes modelos arima que resultaron con mayor grado de significancia en sus coeficientes:
```{r}
specs_arima
```

```{r, echo=FALSE}
model_arima <- arima212
prediccion <- forecast(model_arima, h = 11, level = 95)

#error de la predicción
error <- abs(prediccion$mean - data_imae[265:275])

pronostico <- data.frame("Periodo" = data_excel$Periodo[265:275],
                         "Observado" = data_imae[265:275], 
                         "Prediccion" = prediccion$mean, 
                         "LI 95%" = as.numeric(prediccion$lower),
                         "LS 95%" = as.numeric(prediccion$upper),
                         "Error" = as.numeric(error))
```

Podemos observar que la diferencia entre los modelos no es muy significativa, destacando un poco el ARIMA(2,1,2) y el ARIMA(4,1,4).

Predicción para el último año:
```{r}
pronostico
graficarPred(data_imae, model_arima, prediccion)


```

